{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd9bfee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, Iterable, List\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mboto3\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbotocore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ClientError\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import json\n",
    "from typing import Any, Optional, Iterable, List\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Configuration (your paths)\n",
    "# =========================\n",
    "BUCKET = \"cubixchicagodata\"\n",
    "\n",
    "# Incoming RAW JSON (written by the Extract lambda)\n",
    "RAW_TAXI_PREFIX    = \"rawdata/to_processed/taxi_data/\"\n",
    "RAW_WEATHER_PREFIX = \"rawdata/to_processed/weather_data/\"\n",
    "\n",
    "# Where we move processed RAW JSON (to keep the inbox clean)\n",
    "PROCESSED_TAXI_RAW_PREFIX    = \"rawdata/processede/taxi_data/\"\n",
    "PROCESSED_WEATHER_RAW_PREFIX = \"rawdata/processede/weather_data/\"\n",
    "\n",
    "# Final, transformed CSVs\n",
    "TX_TRANSFORMED_PREFIX = \"transformed_data/taxi_trips/\"\n",
    "WX_TRANSFORMED_PREFIX = \"transformed_data/weather/\"\n",
    "\n",
    "# Master tables (slowly growing dimensions)\n",
    "PAYMENT_MASTER_PREFIX = \"transformed_data/payment_type/\"\n",
    "COMPANY_MASTER_PREFIX = \"transformed_data/company/\"\n",
    "PAYMENT_MASTER_FILE   = \"payment_type_master.csv\"\n",
    "COMPANY_MASTER_FILE   = \"company_master.csv\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# S3 helpers\n",
    "# =========================\n",
    "\n",
    "def read_json_from_s3(bucket: str, key: str) -> Any:\n",
    "    \"\"\"\n",
    "    Read a JSON object from S3 and return it as a Python object (dict or list).\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return json.loads(resp[\"Body\"].read())\n",
    "\n",
    "\n",
    "def read_csv_from_s3(bucket: str, prefix: str, filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a CSV from S3 given 'prefix' + 'filename'.\n",
    "    Raises ClientError if the object does not exist.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    key = f\"{prefix}{filename}\"\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    text = obj[\"Body\"].read().decode(\"utf-8\")\n",
    "    return pd.read_csv(StringIO(text))\n",
    "\n",
    "\n",
    "def upload_dataframe_to_s3(bucket: str, df: pd.DataFrame, key: str) -> None:\n",
    "    \"\"\"\n",
    "    Upload a pandas DataFrame to S3 as CSV to the exact 'key'.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    buf = StringIO()\n",
    "    df.to_csv(buf, index=False)\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=buf.getvalue())\n",
    "\n",
    "\n",
    "def upload_master_data_to_s3(bucket: str, folder: str, file_type: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Overwrite the current master CSV and archive the previous version:\n",
    "    saved as transformed_data/master_table_previous_version/{file_type}_master_previous_version.csv\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    master_key = f\"{folder}{file_type}_master.csv\"\n",
    "    prev_key   = f\"transformed_data/master_table_previous_version/{file_type}_master_previous_version.csv\"\n",
    "\n",
    "    # Try to copy the current master to \"previous\" (if it exists).\n",
    "    try:\n",
    "        s3.copy_object(Bucket=bucket, CopySource={\"Bucket\": bucket, \"Key\": master_key}, Key=prev_key)\n",
    "    except ClientError as e:\n",
    "        # If there is no current master yet, ignore. Re-raise anything else.\n",
    "        if e.response.get(\"Error\", {}).get(\"Code\") not in (\"NoSuchKey\", \"404\"):\n",
    "            raise\n",
    "\n",
    "    # Write the new master\n",
    "    upload_dataframe_to_s3(bucket=bucket, df=df, key=master_key)\n",
    "\n",
    "\n",
    "def list_json_keys(bucket: str, prefix: str) -> Iterable[str]:\n",
    "    \"\"\"\n",
    "    Paginate through S3 and yield all object keys under 'prefix' that end with .json.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for item in page.get(\"Contents\", []):\n",
    "            key = item[\"Key\"]\n",
    "            if key.lower().endswith(\".json\") and not key.endswith(\"/\"):\n",
    "                yield key\n",
    "\n",
    "\n",
    "def _extract_date_from_filename(filename: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try to extract YYYY-MM-DD from a filename shaped like '..._YYYY-MM-DD.json'.\n",
    "    Returns None if it can't find one.\n",
    "    \"\"\"\n",
    "    stem = filename.rsplit(\"/\", 1)[-1].split(\".\")[0]\n",
    "    parts = stem.split(\"_\")\n",
    "    return parts[-1] if parts else None\n",
    "\n",
    "\n",
    "def move_raw_and_write_transformed(\n",
    "    df: pd.DataFrame,\n",
    "    datetime_col: str,\n",
    "    bucket: str,\n",
    "    file_type: str,               # \"taxi\" or \"weather\"\n",
    "    raw_filename: str,            # just the filename, not full key\n",
    "    source_prefix: str,           # e.g. RAW_TAXI_PREFIX\n",
    "    target_raw_prefix: str,       # e.g. PROCESSED_TAXI_RAW_PREFIX\n",
    "    transformed_prefix: str       # e.g. TX_TRANSFORMED_PREFIX\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    1) Write the transformed DataFrame to S3 under transformed_prefix with a date in the filename.\n",
    "       The date is derived from the first non-null value in 'datetime_col' OR\n",
    "       falls back to the date token found in 'raw_filename'.\n",
    "    2) Move the RAW JSON from source_prefix to target_raw_prefix (copy + delete).\n",
    "\n",
    "    Returns the S3 key of the transformed CSV if one was written, otherwise None.\n",
    "    \"\"\"\n",
    "    # Determine date token for the output filename\n",
    "    date_str = None\n",
    "    if isinstance(df, pd.DataFrame) and not df.empty and datetime_col in df.columns:\n",
    "        first_valid = pd.to_datetime(df[datetime_col], errors=\"coerce\").dropna()\n",
    "        if not first_valid.empty:\n",
    "            date_str = first_valid.iloc[0].strftime(\"%Y-%m-%d\")\n",
    "    if not date_str:\n",
    "        date_str = _extract_date_from_filename(raw_filename) or \"unknown_date\"\n",
    "\n",
    "    transformed_key = f\"{transformed_prefix}{file_type}_{date_str}.csv\"\n",
    "    upload_dataframe_to_s3(bucket=bucket, df=df, key=transformed_key)\n",
    "\n",
    "    # Move RAW JSON out of the to_processed inbox\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    old_key = f\"{source_prefix}{raw_filename}\"\n",
    "    new_key = f\"{target_raw_prefix}{raw_filename}\"\n",
    "    s3.copy_object(Bucket=bucket, CopySource={\"Bucket\": bucket, \"Key\": old_key}, Key=new_key)\n",
    "    s3.delete_object(Bucket=bucket, Key=old_key)\n",
    "\n",
    "    print(f\"OK -> {transformed_key}\")\n",
    "    print(f\"MOVED RAW -> {new_key}\")\n",
    "    return transformed_key\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Taxi transforms\n",
    "# =========================\n",
    "\n",
    "def taxi_trips_transformations(taxi_trips: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimal cleanup for taxi trips:\n",
    "    - Drop large/unused geometry columns if present\n",
    "    - Drop fully empty rows\n",
    "    - Normalize column names (community area IDs)\n",
    "    - Add hourly-rounded 'datetime_for_weather' from 'trip_start_timestamp'\n",
    "    \"\"\"\n",
    "    if not isinstance(taxi_trips, pd.DataFrame):\n",
    "        raise TypeError(\"taxi_trips must be a pandas DataFrame\")\n",
    "\n",
    "    # Drop heavy or unused columns if they exist\n",
    "    drop_cols = [\n",
    "        \"pickup_census_tract\", \"dropoff_census_tract\",\n",
    "        \"pickup_centroid_location\", \"dropoff_centroid_location\"\n",
    "    ]\n",
    "    to_drop = [c for c in drop_cols if c in taxi_trips.columns]\n",
    "    if to_drop:\n",
    "        taxi_trips.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # Remove empty rows\n",
    "    taxi_trips.dropna(how=\"all\", inplace=True)\n",
    "\n",
    "    # Normalize column names if source uses different naming\n",
    "    rename_map = {}\n",
    "    if \"pickup_community_area\" in taxi_trips.columns:\n",
    "        rename_map[\"pickup_community_area\"] = \"pickup_community_area_id\"\n",
    "    if \"dropoff_community_area\" in taxi_trips.columns:\n",
    "        rename_map[\"dropoff_community_area\"] = \"dropoff_community_area_id\"\n",
    "    if rename_map:\n",
    "        taxi_trips.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Create hourly-rounded timestamp for joining to weather later\n",
    "    if \"trip_start_timestamp\" in taxi_trips.columns:\n",
    "        taxi_trips[\"datetime_for_weather\"] = (\n",
    "            pd.to_datetime(taxi_trips[\"trip_start_timestamp\"], errors=\"coerce\")\n",
    "              .dt.floor(\"h\")\n",
    "        )\n",
    "\n",
    "    return taxi_trips\n",
    "\n",
    "\n",
    "def update_master(source_df: pd.DataFrame, master_df: pd.DataFrame, id_col: str, value_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Grow a small master table (e.g. company, payment_type) with any new values found in source_df[value_col].\n",
    "    If no new values are found, returns master_df unchanged.\n",
    "    \"\"\"\n",
    "    if master_df is None or master_df.empty:\n",
    "        master_df = pd.DataFrame(columns=[id_col, value_col])\n",
    "\n",
    "    if value_col not in source_df.columns:\n",
    "        return master_df\n",
    "\n",
    "    max_id = int(master_df[id_col].max()) if not master_df.empty else 0\n",
    "    existing = set(master_df[value_col].astype(str).tolist())\n",
    "\n",
    "    candidates = [\n",
    "        v for v in source_df[value_col].astype(str).dropna().unique().tolist()\n",
    "        if v not in existing\n",
    "    ]\n",
    "    if not candidates:\n",
    "        return master_df\n",
    "\n",
    "    new_rows = pd.DataFrame({\n",
    "        id_col: range(max_id + 1, max_id + 1 + len(candidates)),\n",
    "        value_col: candidates\n",
    "    })\n",
    "    return pd.concat([master_df, new_rows], ignore_index=True)\n",
    "\n",
    "\n",
    "def attach_master_ids(taxi_df: pd.DataFrame,\n",
    "                      payment_master: pd.DataFrame,\n",
    "                      company_master: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Left-join the master IDs back to the taxi fact (pandas merge).\n",
    "    Requires ('payment_type_id','payment_type') and ('company_id','company') columns in master tables.\n",
    "    \"\"\"\n",
    "    out = taxi_df.copy()\n",
    "\n",
    "    if (isinstance(payment_master, pd.DataFrame) and not payment_master.empty\n",
    "        and {\"payment_type_id\", \"payment_type\"}.issubset(payment_master.columns)\n",
    "        and \"payment_type\" in out.columns):\n",
    "        out = out.merge(\n",
    "            payment_master[[\"payment_type_id\", \"payment_type\"]],\n",
    "            on=\"payment_type\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    if (isinstance(company_master, pd.DataFrame) and not company_master.empty\n",
    "        and {\"company_id\", \"company\"}.issubset(company_master.columns)\n",
    "        and \"company\" in out.columns):\n",
    "        out = out.merge(\n",
    "            company_master[[\"company_id\", \"company\"]],\n",
    "            on=\"company\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Weather transform\n",
    "# =========================\n",
    "\n",
    "def transform_weather_data(weather_json: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten Open-Meteo hourly arrays into a tabular DataFrame with:\n",
    "    datetime, temperature, wind_speed, rain, precipitation\n",
    "    \"\"\"\n",
    "    hourly = weather_json.get(\"hourly\", {})\n",
    "    table = {\n",
    "        \"datetime\":      hourly.get(\"time\", []),\n",
    "        \"temperature\":   hourly.get(\"temperature_2m\", []),\n",
    "        \"wind_speed\":    hourly.get(\"wind_speed_10m\", []),\n",
    "        \"rain\":          hourly.get(\"rain\", []),\n",
    "        \"precipitation\": hourly.get(\"precipitation\", []),\n",
    "    }\n",
    "    df = pd.DataFrame(table)\n",
    "    if \"datetime\" in df.columns:\n",
    "        df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================\n",
    "# JSON → DataFrame utility\n",
    "# =========================\n",
    "\n",
    "def json_to_dataframe(payload: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flexible JSON → DataFrame:\n",
    "    - list → DataFrame(list)\n",
    "    - dict with key 'data' being a list → DataFrame(dict['data'])\n",
    "    - plain dict → single-row DataFrame\n",
    "    - otherwise → empty DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(payload, list):\n",
    "        return pd.DataFrame(payload)\n",
    "    if isinstance(payload, dict):\n",
    "        if \"data\" in payload and isinstance(payload[\"data\"], list):\n",
    "            return pd.DataFrame(payload[\"data\"])\n",
    "        return pd.DataFrame([payload])\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "def looks_like_error_payload(payload: Any, df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: Socrata sometimes returns a small dict like {'error': ..., 'message': ...}.\n",
    "    Treat such 1-row/2-column frames as an error placeholder, not real data.\n",
    "    \"\"\"\n",
    "    if isinstance(payload, dict):\n",
    "        keys = set(payload.keys())\n",
    "        if keys.issubset({\"error\", \"message\"}) and 0 < len(keys) <= 2:\n",
    "            return True\n",
    "    # Also catch the DataFrame shape (1 row, columns only error/message)\n",
    "    if isinstance(df, pd.DataFrame) and not df.empty:\n",
    "        cols = set(df.columns)\n",
    "        if cols.issubset({\"error\", \"message\"}) and df.shape[1] <= 2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Lambda entrypoint\n",
    "# =========================\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Transform & Load pipeline:\n",
    "      - Reads RAW JSON from S3 inbox (rawdata/to_processed/...)\n",
    "      - Transforms into normalized CSVs (transformed_data/...)\n",
    "      - Moves the RAW files out of the inbox (rawdata/processede/...)\n",
    "      - Updates/archives master tables (payment_type/company)\n",
    "    This handler is safe against S3 recursion: it ignores events not under 'rawdata/to_processed/'.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "\n",
    "    # --- Recursion guard: only react to keys under rawdata/to_processed/\n",
    "    if event and \"Records\" in event:\n",
    "        for rec in event[\"Records\"]:\n",
    "            key = rec[\"s3\"][\"object\"][\"key\"]\n",
    "            if not key.startswith(\"rawdata/to_processed/\"):\n",
    "                print(f\"SKIP {key}: outside of rawdata/to_processed/.\")\n",
    "                return {\"statusCode\": 200, \"body\": json.dumps({\"skipped\": key})}\n",
    "\n",
    "    # --- Load current masters (or start with empty frames)\n",
    "    try:\n",
    "        payment_master = read_csv_from_s3(BUCKET, PAYMENT_MASTER_PREFIX, PAYMENT_MASTER_FILE)\n",
    "    except ClientError:\n",
    "        payment_master = pd.DataFrame(columns=[\"payment_type_id\", \"payment_type\"])\n",
    "\n",
    "    try:\n",
    "        company_master = read_csv_from_s3(BUCKET, COMPANY_MASTER_PREFIX, COMPANY_MASTER_FILE)\n",
    "    except ClientError:\n",
    "        company_master = pd.DataFrame(columns=[\"company_id\", \"company\"])\n",
    "\n",
    "    written_keys: List[str] = []\n",
    "\n",
    "    # ============== TAXI ==============\n",
    "    for key in list_json_keys(BUCKET, RAW_TAXI_PREFIX):\n",
    "        filename = key.split(\"/\")[-1]\n",
    "        payload = read_json_from_s3(BUCKET, key)\n",
    "        taxi_raw_df = json_to_dataframe(payload)\n",
    "\n",
    "        # If this looks like an API error placeholder, move RAW and skip writing transformed.\n",
    "        if taxi_raw_df.empty or looks_like_error_payload(payload, taxi_raw_df):\n",
    "            print(f\"SKIP transformed for {key}: empty/error payload.\")\n",
    "            # Still move RAW out of the inbox to avoid re-processing\n",
    "            move_raw_and_write_transformed(\n",
    "                df=pd.DataFrame(),  # write an empty CSV only if you really want; else skip writing by not calling\n",
    "                datetime_col=\"datetime_for_weather\",\n",
    "                bucket=BUCKET,\n",
    "                file_type=\"taxi\",\n",
    "                raw_filename=filename,\n",
    "                source_prefix=RAW_TAXI_PREFIX,\n",
    "                target_raw_prefix=PROCESSED_TAXI_RAW_PREFIX,\n",
    "                transformed_prefix=TX_TRANSFORMED_PREFIX\n",
    "            )\n",
    "            # NOTE: If you do NOT want an empty transformed CSV, remove the call above\n",
    "            # and just copy+delete the RAW yourself here.\n",
    "            continue\n",
    "\n",
    "        # Clean & normalize taxi data\n",
    "        taxi_transformed = taxi_trips_transformations(taxi_raw_df)\n",
    "        print(\"TAXI columns:\", list(taxi_transformed.columns))\n",
    "\n",
    "        # Grow masters only if the columns exist\n",
    "        if \"company\" in taxi_transformed.columns:\n",
    "            company_master = update_master(taxi_transformed, company_master, \"company_id\", \"company\")\n",
    "        else:\n",
    "            print(\"SKIP master update: no 'company' column.\")\n",
    "        if \"payment_type\" in taxi_transformed.columns:\n",
    "            payment_master = update_master(taxi_transformed, payment_master, \"payment_type_id\", \"payment_type\")\n",
    "        else:\n",
    "            print(\"SKIP master update: no 'payment_type' column.\")\n",
    "\n",
    "        # Attach IDs from masters back to fact\n",
    "        taxi_with_ids = attach_master_ids(taxi_transformed, payment_master, company_master)\n",
    "\n",
    "        # Write transformed CSV and move RAW JSON out of the inbox\n",
    "        out_key = move_raw_and_write_transformed(\n",
    "            df=taxi_with_ids,\n",
    "            datetime_col=\"datetime_for_weather\",\n",
    "            bucket=BUCKET,\n",
    "            file_type=\"taxi\",\n",
    "            raw_filename=filename,\n",
    "            source_prefix=RAW_TAXI_PREFIX,\n",
    "            target_raw_prefix=PROCESSED_TAXI_RAW_PREFIX,\n",
    "            transformed_prefix=TX_TRANSFORMED_PREFIX\n",
    "        )\n",
    "        if out_key:\n",
    "            written_keys.append(out_key)\n",
    "\n",
    "    # Persist updated masters (with archival of previous version)\n",
    "    upload_master_data_to_s3(BUCKET, PAYMENT_MASTER_PREFIX, \"payment_type\", payment_master)\n",
    "    upload_master_data_to_s3(BUCKET, COMPANY_MASTER_PREFIX, \"company\", company_master)\n",
    "    print(\"payment_type_master updated.\")\n",
    "    print(\"company_master updated.\")\n",
    "\n",
    "    # ============= WEATHER =============\n",
    "    for key in list_json_keys(BUCKET, RAW_WEATHER_PREFIX):\n",
    "        filename = key.split(\"/\")[-1]\n",
    "        payload = read_json_from_s3(BUCKET, key)\n",
    "        weather_df = transform_weather_data(payload)\n",
    "\n",
    "        # If weather is empty, still move the RAW to avoid retries\n",
    "        if weather_df.empty:\n",
    "            print(f\"SKIP transformed for {key}: empty weather frame.\")\n",
    "            move_raw_and_write_transformed(\n",
    "                df=pd.DataFrame(),  # write empty CSV only if you want one\n",
    "                datetime_col=\"datetime\",\n",
    "                bucket=BUCKET,\n",
    "                file_type=\"weather\",\n",
    "                raw_filename=filename,\n",
    "                source_prefix=RAW_WEATHER_PREFIX,\n",
    "                target_raw_prefix=PROCESSED_WEATHER_RAW_PREFIX,\n",
    "                transformed_prefix=WX_TRANSFORMED_PREFIX\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        out_key = move_raw_and_write_transformed(\n",
    "            df=weather_df,\n",
    "            datetime_col=\"datetime\",\n",
    "            bucket=BUCKET,\n",
    "            file_type=\"weather\",\n",
    "            raw_filename=filename,\n",
    "            source_prefix=RAW_WEATHER_PREFIX,\n",
    "            target_raw_prefix=PROCESSED_WEATHER_RAW_PREFIX,\n",
    "            transformed_prefix=WX_TRANSFORMED_PREFIX\n",
    "        )\n",
    "        if out_key:\n",
    "            written_keys.append(out_key)\n",
    "\n",
    "    # Final response (handy in test console)\n",
    "    return {\"statusCode\": 200, \"body\": json.dumps({\"written\": written_keys})}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
